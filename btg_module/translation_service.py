# translation_service.py
import time
import random
import re
import csv
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
import os
import json # JSON ëª¨ë“ˆ ì„í¬íŠ¸

try:
    from .gemini_client import (
        GeminiClient,
        GeminiContentSafetyException,
        GeminiRateLimitException,
        GeminiApiException,
        GeminiInvalidRequestException,
        GeminiAllApiKeysExhaustedException 
    )
    from .file_handler import read_json_file, read_text_file # read_text_fileë„ í•„ìš”í•  ìˆ˜ ìˆìŒ (ìŠ¤í‚¤ë§ˆ íŒŒì¼ì´ ì¼ë°˜ í…ìŠ¤íŠ¸ì¼ ê²½ìš°)
    from .logger_config import setup_logger
    from .exceptions import BtgTranslationException, BtgApiClientException
    from .chunk_service import ChunkService
    # types ëª¨ë“ˆì€ gemini_clientì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì§ì ‘ì ì¸ ì˜ì¡´ì„±ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ë§Œì•½ ì´ íŒŒì¼ ë‚´ì—ì„œ types.Part ë“±ì„ ì§ì ‘ ì‚¬ìš©í•œë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ì„í¬íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    # from google.genai import types as genai_types 
    from .dtos import LorebookEntryDTO # ë¡œì–´ë¶ DTO ì„í¬íŠ¸
except ImportError:
    from gemini_client import (
        GeminiClient,
        GeminiContentSafetyException,
        GeminiRateLimitException,
        GeminiApiException,
        GeminiInvalidRequestException,
        GeminiAllApiKeysExhaustedException 
    )
    from file_handler import read_json_file, read_text_file
    from logger_config import setup_logger
    from exceptions import BtgTranslationException, BtgApiClientException
    from chunk_service import ChunkService
    from dtos import LorebookEntryDTO
    from google.genai import types as genai_types # Function Callingì„ ìœ„í•´ ì¶”ê°€
    # from google.genai import types as genai_types # Fallback import

logger = setup_logger(__name__)

def _format_lorebook_for_prompt(
    lorebook_entries: List[LorebookEntryDTO],
    max_entries: int,
    max_chars: int
) -> str:
    if not lorebook_entries:
        return "ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"

    selected_entries_str = []
    current_chars = 0
    entries_count = 0

    # ì¤‘ìš”ë„ ë†’ì€ ìˆœ, ì¤‘ìš”ë„ ê°™ìœ¼ë©´ í‚¤ì›Œë“œ ê°€ë‚˜ë‹¤ ìˆœìœ¼ë¡œ ì •ë ¬
    # isSpoilerê°€ Trueì¸ í•­ëª©ì€ ë‚®ì€ ìš°ì„ ìˆœìœ„ë¥¼ ê°–ë„ë¡ ì¡°ì • (ì˜ˆ: ì¤‘ìš”ë„ë¥¼ ë‚®ì¶¤)
    def sort_key(entry: LorebookEntryDTO):
        importance = entry.importance or 0
        if entry.isSpoiler:
            importance -= 100 # ìŠ¤í¬ì¼ëŸ¬ í•­ëª©ì˜ ì¤‘ìš”ë„ë¥¼ í¬ê²Œ ë‚®ì¶¤
        return (-importance, entry.keyword.lower())

    sorted_entries = sorted(lorebook_entries, key=sort_key)

    for entry in sorted_entries:
        if entries_count >= max_entries:
            break

        spoiler_text = "ì˜ˆ" if entry.isSpoiler else "ì•„ë‹ˆì˜¤"
        details_parts = []
        if entry.category:
            details_parts.append(f"ì¹´í…Œê³ ë¦¬: {entry.category}")
        details_parts.append(f"ìŠ¤í¬ì¼ëŸ¬: {spoiler_text}")
        
        details_str = ", ".join(details_parts)
        # ë¡œì–´ë¶ í•­ëª©ì˜ ì›ë³¸ ì–¸ì–´ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        lang_info = f" (lang: {entry.source_language})" if entry.source_language else ""
        entry_str = f"- {entry.keyword}{lang_info}: {entry.description} ({details_str})"
        
        
        
        # í˜„ì¬ í•­ëª© ì¶”ê°€ ì‹œ ìµœëŒ€ ê¸€ì ìˆ˜ ì´ˆê³¼í•˜ë©´ ì¤‘ë‹¨ (ë‹¨, ìµœì†Œ 1ê°œëŠ” í¬í•¨ë˜ë„ë¡)
        if current_chars + len(entry_str) > max_chars and entries_count > 0:
            break
        
        selected_entries_str.append(entry_str)
        current_chars += len(entry_str) + 1 # +1 for newline
        entries_count += 1
    
    if not selected_entries_str:
        return "ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ (ì œí•œìœ¼ë¡œ ì¸í•´ ì„ íƒëœ í•­ëª© ì—†ìŒ)"
        
    return "\n".join(selected_entries_str)

class TranslationService:
    def __init__(self, gemini_client: GeminiClient, config: Dict[str, Any]):
        self.gemini_client = gemini_client
        self.config = config
        self.chunk_service = ChunkService()
        self.lorebook_entries_for_injection: List[LorebookEntryDTO] = [] # For new lorebook injection

        if self.config.get("enable_dynamic_lorebook_injection", False):
            self._load_lorebook_data()
            logger.info("ë™ì  ë¡œì–´ë¶ ì£¼ì… í™œì„±í™”ë¨. ë¡œì–´ë¶ ë°ì´í„° ë¡œë“œ ì‹œë„.")
        else:
            logger.info("ë™ì  ë¡œì–´ë¶ ì£¼ì… ë¹„í™œì„±í™”ë¨. ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ë²ˆì—­í•©ë‹ˆë‹¤.")

    def _load_lorebook_data(self):
        # í†µí•©ëœ ë¡œì–´ë¶ ê²½ë¡œ ì‚¬ìš©
        lorebook_json_path_str = self.config.get("lorebook_json_path")
        if lorebook_json_path_str and os.path.exists(lorebook_json_path_str):
            lorebook_json_path = Path(lorebook_json_path_str)
            try:
                raw_data = read_json_file(lorebook_json_path)
                if isinstance(raw_data, list):
                    for item_dict in raw_data:
                        if isinstance(item_dict, dict) and "keyword" in item_dict and "description" in item_dict:
                            try:
                                entry = LorebookEntryDTO(
                                    keyword=item_dict.get("keyword", ""),
                                    description=item_dict.get("description", ""),
                                    category=item_dict.get("category"),
                                    importance=int(item_dict.get("importance", 0)) if item_dict.get("importance") is not None else None,
                                    sourceSegmentTextPreview=item_dict.get("sourceSegmentTextPreview"),
                                    isSpoiler=bool(item_dict.get("isSpoiler", False)),
                                    source_language=item_dict.get("source_language") # ë¡œì–´ë¶ JSONì—ì„œ source_language ë¡œë“œ
                                )
                                if entry.keyword and entry.description: # í•„ìˆ˜ í•„ë“œ í™•ì¸
                                    self.lorebook_entries_for_injection.append(entry)
                                else:
                                    logger.warning(f"ë¡œì–´ë¶ í•­ëª©ì— í•„ìˆ˜ í•„ë“œ(keyword ë˜ëŠ” description) ëˆ„ë½: {item_dict}")
                            except (TypeError, ValueError) as e_dto:
                                logger.warning(f"ë¡œì–´ë¶ í•­ëª© DTO ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {item_dict}, ì˜¤ë¥˜: {e_dto}")
                        else:
                            logger.warning(f"ì˜ëª»ëœ ë¡œì–´ë¶ í•­ëª© í˜•ì‹ (ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆê±°ë‚˜ í•„ìˆ˜ í‚¤ ëˆ„ë½): {item_dict}")
                    logger.info(f"{len(self.lorebook_entries_for_injection)}ê°œì˜ ë¡œì–´ë¶ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {lorebook_json_path}")
                else:
                    logger.error(f"ë¡œì–´ë¶ JSON íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {lorebook_json_path}, íƒ€ì…: {type(raw_data)}")
            except Exception as e:
                logger.error(f"ë¡œì–´ë¶ JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({lorebook_json_path}): {e}", exc_info=True)
                self.lorebook_entries_for_injection = []
        else:
            logger.info(f"ë¡œì–´ë¶ JSON íŒŒì¼({lorebook_json_path_str})ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë™ì  ì£¼ì…ì„ ìœ„í•´ ë¡œì–´ë¶ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            self.lorebook_entries_for_injection = []

    def _construct_prompt(self, chunk_text: str) -> str:
        prompt_template = self.config.get("prompts", "Translate to Korean: {{slot}}")
        if isinstance(prompt_template, (list, tuple)):
            prompt_template = prompt_template[0] if prompt_template else "Translate to Korean: {{slot}}"

        final_prompt = prompt_template

        # Determine the source language for the current chunk to filter lorebook entries
        config_source_lang = self.config.get("novel_language") # í†µí•©ëœ ì„¤ì • ì‚¬ìš©
        # Fallback language from config, with a hardcoded default if the config key itself is missing
        config_fallback_lang = self.config.get("novel_language_fallback", "ja") # í†µí•©ëœ í´ë°± ì„¤ì • ì‚¬ìš©

        # "auto" ëª¨ë“œì¼ ë•Œ, LLMì´ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  ë¡œì–´ë¶ì„ í•„í„°ë§í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ê°€ êµ¬ì„±ë©ë‹ˆë‹¤.
        # Python ë‹¨ì—ì„œ current_source_lang_for_translationì„ í™•ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ë¡œê¹…ì´ë‚˜ íŠ¹ì • ì¡°ê±´ë¶€ ë¡œì§ì„ ìœ„í•´ì„  ì—¬ì „íˆ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, ë¡œì–´ë¶ í•„í„°ë§ì€ LLMìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
        current_source_lang_for_lorebook_filtering: Optional[str] = None

        if config_source_lang == "auto":
            logger.info(f"ë²ˆì—­ ì¶œë°œ ì–¸ì–´ ì„¤ì •: 'auto'. LLMì´ í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  ë¡œì–´ë¶ì„ ì ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.")
            # current_source_lang_for_lorebook_filteringëŠ” Noneìœ¼ë¡œ ìœ ì§€í•˜ê±°ë‚˜ "auto"ë¡œ ì„¤ì •.
            # ë¡œì–´ë¶ í•„í„°ë§ì€ LLMì˜ ì—­í• ì´ ë©ë‹ˆë‹¤.
        elif config_source_lang and isinstance(config_source_lang, str) and config_source_lang.strip(): # Specific language code provided
            current_source_lang_for_lorebook_filtering = config_source_lang
            logger.info(f"ëª…ì‹œì  ë²ˆì—­ ì¶œë°œ ì–¸ì–´ '{current_source_lang_for_lorebook_filtering}' ì‚¬ìš©. ë¡œì–´ë¶ë„ ì´ ì–¸ì–´ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ë©ë‹ˆë‹¤.")
        else: # config_source_lang is None, empty string, or not "auto"
            current_source_lang_for_lorebook_filtering = config_fallback_lang
            logger.warning(f"ë²ˆì—­ ì¶œë°œ ì–¸ì–´ê°€ ìœ íš¨í•˜ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ 'auto'ê°€ ì•„ë‹™ë‹ˆë‹¤. í´ë°± ì–¸ì–´ '{current_source_lang_for_lorebook_filtering}'ë¥¼ ë¡œì–´ë¶ í•„í„°ë§ì— ì‚¬ìš©.")

        # 1. Dynamic Lorebook Injection
        if self.config.get("enable_dynamic_lorebook_injection", False) and \
           self.lorebook_entries_for_injection and \
           "{{lorebook_context}}" in final_prompt:

            relevant_entries_for_chunk: List[LorebookEntryDTO] = []
            chunk_text_lower = chunk_text.lower() # For case-insensitive keyword matching

            if config_source_lang == "auto":
                # "auto" ëª¨ë“œ: LLMì´ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  ë¡œì–´ë¶ì„ í•„í„°ë§í•˜ë„ë¡ ì§€ì‹œ.
                # Pythonì—ì„œëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œë§Œ í•„í„°ë§í•˜ê±°ë‚˜, ëª¨ë“  ë¡œì–´ë¶ í•­ëª©ì„ ì „ë‹¬.
                # ì—¬ê¸°ì„œëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ë§Œ ìˆ˜í–‰í•˜ê³ , LLMì´ ì–¸ì–´ í•„í„°ë§ì„ í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œ.
                logger.info("ìë™ ì–¸ì–´ ê°ì§€ ëª¨ë“œ: ë¡œì–´ë¶ì€ í‚¤ì›Œë“œ ì¼ì¹˜ë¡œ í•„í„°ë§ í›„ LLMì— ì „ë‹¬. LLMì´ ì–¸ì–´ ê¸°ë°˜ ì¶”ê°€ í•„í„°ë§ ìˆ˜í–‰.")
                for entry in self.lorebook_entries_for_injection:
                    if entry.keyword.lower() in chunk_text_lower:
                        relevant_entries_for_chunk.append(entry)
            else:
                # ëª…ì‹œì  ì–¸ì–´ ì„¤ì • ëª¨ë“œ: Pythonì—ì„œ ì–¸ì–´ ë° í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§.
                logger.info(f"ëª…ì‹œì  ì–¸ì–´ ëª¨ë“œ ('{current_source_lang_for_lorebook_filtering}'): ë¡œì–´ë¶ì„ ì–¸ì–´ ë° í‚¤ì›Œë“œ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§.")
                for entry in self.lorebook_entries_for_injection:
                    # ë¡œì–´ë¶ í•­ëª©ì˜ ì–¸ì–´ì™€ í˜„ì¬ ë²ˆì—­ ì¶œë°œ ì–¸ì–´ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if entry.source_language and \
                       current_source_lang_for_lorebook_filtering and \
                       entry.source_language.lower() != current_source_lang_for_lorebook_filtering.lower():
                        logger.debug(f"ë¡œì–´ë¶ í•­ëª© '{entry.keyword}' ê±´ë„ˆëœ€: ì–¸ì–´ ë¶ˆì¼ì¹˜ (ë¡œì–´ë¶: {entry.source_language}, ë²ˆì—­ ì¶œë°œ: {current_source_lang_for_lorebook_filtering}).")
                        continue

                    if entry.keyword.lower() in chunk_text_lower:
                        relevant_entries_for_chunk.append(entry)
            
            logger.debug(f"í˜„ì¬ ì²­í¬ì— ëŒ€í•´ {len(relevant_entries_for_chunk)}ê°œì˜ ê´€ë ¨ ë¡œì–´ë¶ í•­ëª© ë°œê²¬.")

            # 1.b. Format the relevant entries for the prompt
            max_entries = self.config.get("max_lorebook_entries_per_chunk_injection", 3)
            max_chars = self.config.get("max_lorebook_chars_per_chunk_injection", 500)
            
            formatted_lorebook_context = _format_lorebook_for_prompt(
                relevant_entries_for_chunk, max_entries, max_chars # Pass only relevant entries
            )
            
            # Check if actual content was formatted (not just "ì—†ìŒ" messages)
            if formatted_lorebook_context != "ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ" and \
               formatted_lorebook_context != "ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ (ì œí•œìœ¼ë¡œ ì¸í•´ ì„ íƒëœ í•­ëª© ì—†ìŒ)":
                logger.info(f"API ìš”ì²­ì— ë™ì  ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ë¨. ë‚´ìš© ì¼ë¶€: {formatted_lorebook_context[:100]}...")
                # ì£¼ì…ëœ ë¡œì–´ë¶ í‚¤ì›Œë“œ ë¡œê¹…
                injected_keywords = [entry.keyword for entry in relevant_entries_for_chunk if entry.keyword.lower() in chunk_text_lower]
                if injected_keywords:
                    logger.info(f"  ğŸ”‘ ì£¼ì…ëœ ë¡œì–´ë¶ í‚¤ì›Œë“œ: {', '.join(injected_keywords)}")
            else:
                logger.debug(f"ë™ì  ë¡œì–´ë¶ ì£¼ì… ì‹œë„í–ˆìœ¼ë‚˜, ê´€ë ¨ í•­ëª© ì—†ê±°ë‚˜ ì œí•œìœ¼ë¡œ ì¸í•´ ì‹¤ì œ ì£¼ì… ë‚´ìš© ì—†ìŒ. ì‚¬ìš©ëœ ë©”ì‹œì§€: {formatted_lorebook_context}")
            final_prompt = final_prompt.replace("{{lorebook_context}}", formatted_lorebook_context)
        else:
            if "{{lorebook_context}}" in final_prompt:
                 final_prompt = final_prompt.replace("{{lorebook_context}}", "ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ (ì£¼ì… ë¹„í™œì„±í™” ë˜ëŠ” í•´ë‹¹ í•­ëª© ì—†ìŒ)")
                 logger.debug("ë™ì  ë¡œì–´ë¶ ì£¼ì… ë¹„í™œì„±í™” ë˜ëŠ” í”Œë ˆì´ìŠ¤í™€ë” ë¶€ì¬ë¡œ 'ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ' ë©”ì‹œì§€ ì‚¬ìš©.")
        
        # 3. Main content slot - This should be done *after* all other placeholders are processed.
        final_prompt = final_prompt.replace("{{slot}}", chunk_text)
        
        return final_prompt

    def translate_text(self, text_chunk: str) -> str:
        """ê¸°ì¡´ translate_text ë©”ì„œë“œ (ìˆ˜ì • ì—†ìŒ)"""
        if not text_chunk.strip():
            return ""

        processed_text = text_chunk
        prompt = self._construct_prompt(processed_text)

        try:
            logger.debug(f"Gemini API í˜¸ì¶œ ì‹œì‘. ëª¨ë¸: {self.config.get('model_name')}")
            
            translated_text = self.gemini_client.generate_text(
                prompt=prompt,
                model_name=self.config.get("model_name", "gemini-2.0-flash"),
                generation_config_dict={
                    "temperature": self.config.get("temperature", 0.7),
                    "top_p": self.config.get("top_p", 0.9)
                },
            )

            if translated_text is None:
                logger.error("GeminiClient.generate_textê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                raise BtgApiClientException("API í˜¸ì¶œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

            logger.debug(f"Gemini API í˜¸ì¶œ ì„±ê³µ. ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (ì¼ë¶€): {translated_text[:100]}...")

        except GeminiContentSafetyException as e_safety:
            logger.warning(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œë¡œ ë²ˆì—­ ì‹¤íŒ¨: {e_safety}")
            raise BtgTranslationException(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œë¡œ ë²ˆì—­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e_safety})", original_exception=e_safety) from e_safety
        except GeminiAllApiKeysExhaustedException as e_keys:
            logger.error(f"API í‚¤ íšŒì „ ì‹¤íŒ¨: ëª¨ë“  API í‚¤ ì†Œì§„ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ. ì›ë³¸ ì˜¤ë¥˜: {e_keys}")
            raise BtgApiClientException(f"ëª¨ë“  API í‚¤ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ ìš”ì²­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”. ({e_keys})", original_exception=e_keys) from e_keys
        except GeminiRateLimitException as e_rate:
            logger.error(f"API ì‚¬ìš©ëŸ‰ ì œí•œ ì´ˆê³¼ (í‚¤ íšŒì „ í›„ì—ë„ ë°œìƒ): {e_rate}")
            raise BtgApiClientException(f"API ì‚¬ìš©ëŸ‰ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ({e_rate})", original_exception=e_rate) from e_rate
        except GeminiInvalidRequestException as e_invalid:
            logger.error(f"ì˜ëª»ëœ API ìš”ì²­: {e_invalid}")
            raise BtgApiClientException(f"ì˜ëª»ëœ API ìš”ì²­ì…ë‹ˆë‹¤: {e_invalid}", original_exception=e_invalid) from e_invalid
        # ì¤‘ë³µëœ GeminiContentSafetyException ì œê±°
        except GeminiApiException as e_api:
            logger.error(f"Gemini API í˜¸ì¶œ ì¤‘ ì¼ë°˜ ì˜¤ë¥˜ ë°œìƒ: {e_api}")
            raise BtgApiClientException(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e_api}", original_exception=e_api) from e_api
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            raise BtgTranslationException(f"ë²ˆì—­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", original_exception=e) from e
        
        final_text = translated_text 
        return final_text.strip()
    
    def _load_response_schema(self, schema_name: str) -> Optional[Dict[str, Any]]:
        """
        ì§€ì •ëœ ì´ë¦„ì˜ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        ìŠ¤í‚¤ë§ˆ íŒŒì¼ì€ ì„¤ì •ëœ 'response_schemas_dir' ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
        """
        schemas_dir_path_str = self.config.get("response_schemas_dir")
        if not schemas_dir_path_str:
            logger.warning("ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ë””ë ‰í† ë¦¬('response_schemas_dir')ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í‚¤ë§ˆë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        schema_file_path = Path(schemas_dir_path_str) / (schema_name if schema_name.endswith(".json") else f"{schema_name}.json")
        
        if not schema_file_path.exists() or not schema_file_path.is_file():
            logger.warning(f"ì‘ë‹µ ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{schema_file_path}'")
            return None
        
        try:
            # file_handler.read_json_fileì€ ì´ë¯¸ JSON íŒŒì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            schema_content = read_json_file(schema_file_path)
            if not isinstance(schema_content, dict): # ìŠ¤í‚¤ë§ˆëŠ” ë³´í†µ dict í˜•íƒœ
                logger.error(f"ë¡œë“œëœ ìŠ¤í‚¤ë§ˆ '{schema_name}'ì˜ ë‚´ìš©ì´ JSON ê°ì²´(dict)ê°€ ì•„ë‹™ë‹ˆë‹¤. íƒ€ì…: {type(schema_content)}")
                return None
            logger.info(f"ì‘ë‹µ ìŠ¤í‚¤ë§ˆ '{schema_name}'ì„(ë¥¼) '{schema_file_path}'ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return schema_content
        except json.JSONDecodeError as e_json:
            logger.error(f"ì‘ë‹µ ìŠ¤í‚¤ë§ˆ íŒŒì¼ '{schema_file_path}' íŒŒì‹± ì¤‘ JSON ì˜¤ë¥˜ ë°œìƒ: {e_json}")
            return None
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìŠ¤í‚¤ë§ˆ íŒŒì¼ '{schema_file_path}' ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return None

    def request_structured_reconstruction(
        self,
        primary_translated_text: str,
        original_html_structure_info: Any,
        response_schema_name: str,
        generation_config_overrides: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Phase 3 (2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸): 1ì°¨ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì™€ ì›ë³¸ HTML êµ¬ì¡° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ
        Gemini APIì— êµ¬ì¡°í™”ëœ ì¬êµ¬ì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤.
        """
        if not self.gemini_client:
            raise BtgTranslationException("GeminiClientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ êµ¬ì¡°í™”ëœ ì¬êµ¬ì„±ì„ ìš”ì²­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        json_schema_definition = self._load_response_schema(response_schema_name)
        if not json_schema_definition:
            logger.warning(f"ìŠ¤í‚¤ë§ˆ '{response_schema_name}' ë¡œë“œ ì‹¤íŒ¨. ìŠ¤í‚¤ë§ˆ ì—†ì´ ì§„í–‰í•˜ê±°ë‚˜ ì˜¤ë¥˜ ì²˜ë¦¬ í•„ìš”.")
            # í•„ìš”ì‹œ ì—¬ê¸°ì„œ ì˜ˆì™¸ ë°œìƒ: raise BtgConfigException(f"ì‘ë‹µ ìŠ¤í‚¤ë§ˆ '{response_schema_name}'ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        ë‹¤ìŒì€ 1ì°¨ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì™€ ì›ë³¸ HTML êµ¬ì¡° ì •ë³´ì…ë‹ˆë‹¤.
        ì´ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì½˜í…ì¸ ë¥¼ ì¬êµ¬ì„±í•˜ì—¬, ì•„ë˜ ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆì— ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”.

        JSON ìŠ¤í‚¤ë§ˆ:
        ```json
        {json.dumps(json_schema_definition, indent=2, ensure_ascii=False) if json_schema_definition else "ì œê³µëœ ìŠ¤í‚¤ë§ˆ ì—†ìŒ. ì¼ë°˜ì ì¸ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."}
        ```

        1ì°¨ ë²ˆì—­ëœ í…ìŠ¤íŠ¸:
        ---
        {primary_translated_text}
        ---

        ì›ë³¸ HTML êµ¬ì¡° ì •ë³´:
        ---
        {json.dumps(original_html_structure_info, indent=2, ensure_ascii=False) if isinstance(original_html_structure_info, (dict, list)) else str(original_html_structure_info)}
        ---

        ìš”ì²­: ìœ„ì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸ ë¥¼ ì¬êµ¬ì„±í•˜ê³ , ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSON ê°ì²´ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSON ê°ì²´ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        """

        effective_gen_config = {
            "temperature": self.config.get("temperature", 0.3), # êµ¬ì¡°í™” ì‘ì—…ì—ëŠ” ë‚®ì€ ì˜¨ë„ê°€ ì í•©í•  ìˆ˜ ìˆìŒ
            "top_p": self.config.get("top_p", 0.9),
            "response_mime_type": "application/json" # JSON ì‘ë‹µ ìš”ì²­
        }
        if generation_config_overrides:
            effective_gen_config.update(generation_config_overrides)

        try:
            logger.info(f"êµ¬ì¡°í™”ëœ ì¬êµ¬ì„± ìš”ì²­ ì‹œì‘ (ìŠ¤í‚¤ë§ˆ: {response_schema_name}).")
            response_data = self.gemini_client.generate_text(
                prompt=prompt,
                model_name=self.config.get("model_name", "gemini-1.5-flash-latest"),
                generation_config_dict=effective_gen_config
            )
            if isinstance(response_data, dict):
                return response_data
            elif isinstance(response_data, str): # GeminiClientê°€ JSON íŒŒì‹±ì— ì‹¤íŒ¨í•œ ê²½ìš°
                logger.warning("GeminiClientê°€ JSON ë¬¸ìì—´ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ (íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” APIê°€ JSON ì•„ë‹Œ ì‘ë‹µ). ì¬íŒŒì‹± ì‹œë„.")
                try:
                    return json.loads(response_data)
                except json.JSONDecodeError as e_json_reparse:
                    raise BtgTranslationException(f"êµ¬ì¡°í™”ëœ ì¬êµ¬ì„± ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e_json_reparse}. ì‘ë‹µ: {response_data[:200]}...", original_exception=e_json_reparse)
            else:
                raise BtgTranslationException(f"êµ¬ì¡°í™”ëœ ì¬êµ¬ì„± ìš”ì²­ì— ëŒ€í•œ ì˜ˆìƒì¹˜ ì•Šì€ ì‘ë‹µ íƒ€ì…: {type(response_data)}")
        except (BtgApiClientException, GeminiAllApiKeysExhaustedException) as e_api:
            logger.error(f"êµ¬ì¡°í™”ëœ ì¬êµ¬ì„± ì¤‘ API ì˜¤ë¥˜: {e_api}", exc_info=True)
            raise # BtgIntegrationServiceì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ê·¸ëŒ€ë¡œ ì „ë‹¬
        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ì¬êµ¬ì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            raise BtgTranslationException(f"êµ¬ì¡°í™”ëœ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", original_exception=e) from e

    def request_direct_structured_translation(
        self,
        content_to_translate: str,
        source_lang: str,
        target_lang: str,
        response_schema_name: str,
        generation_config_overrides: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Phase 3 (ì œí•œì  êµ¬ì¡°í™” ì¶œë ¥): íŠ¹ì • ì½˜í…ì¸ ì— ëŒ€í•´ ì§ì ‘ êµ¬ì¡°í™”ëœ ë²ˆì—­ ì¶œë ¥ì„ ìš”ì²­í•©ë‹ˆë‹¤.
        """
        if not self.gemini_client:
            raise BtgTranslationException("GeminiClientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì§ì ‘ êµ¬ì¡°í™”ëœ ë²ˆì—­ì„ ìš”ì²­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        json_schema_definition = self._load_response_schema(response_schema_name)
        # ìŠ¤í‚¤ë§ˆ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬ (ìœ„ì™€ ìœ ì‚¬)
        if not json_schema_definition:
            logger.warning(f"ìŠ¤í‚¤ë§ˆ '{response_schema_name}' ë¡œë“œ ì‹¤íŒ¨. ìŠ¤í‚¤ë§ˆ ì—†ì´ ì§„í–‰í•˜ê±°ë‚˜ ì˜¤ë¥˜ ì²˜ë¦¬ í•„ìš”.")

        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {source_lang}ì—ì„œ {target_lang}(ìœ¼)ë¡œ ë²ˆì—­í•˜ê³ , ê²°ê³¼ë¥¼ ì•„ë˜ ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆì— ë”°ë¼ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.

        JSON ìŠ¤í‚¤ë§ˆ:
        ```json
        {json.dumps(json_schema_definition, indent=2, ensure_ascii=False) if json_schema_definition else "ì œê³µëœ ìŠ¤í‚¤ë§ˆ ì—†ìŒ. ì¼ë°˜ì ì¸ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."}
        ```

        ë²ˆì—­ ë° êµ¬ì¡°í™”í•  ì›ë³¸ í…ìŠ¤íŠ¸ ({source_lang}):
        ---
        {content_to_translate}
        ---

        ìš”ì²­: ìœ„ì˜ í…ìŠ¤íŠ¸ë¥¼ {target_lang}(ìœ¼)ë¡œ ë²ˆì—­í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆì— ë§ëŠ” JSON ê°ì²´ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ JSON ê°ì²´ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        """

        effective_gen_config = {
            "temperature": self.config.get("temperature", 0.5), # ë²ˆì—­ê³¼ êµ¬ì¡°í™”ë¥¼ ë™ì‹œì— í•˜ë¯€ë¡œ ì•½ê°„ì˜ ìœ ì—°ì„±
            "top_p": self.config.get("top_p", 0.9),
            "response_mime_type": "application/json"
        }
        if generation_config_overrides:
            effective_gen_config.update(generation_config_overrides)

        try:
            logger.info(f"ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­ ìš”ì²­ ì‹œì‘ (ìŠ¤í‚¤ë§ˆ: {response_schema_name}, {source_lang} -> {target_lang}).")
            response_data = self.gemini_client.generate_text(
                prompt=prompt,
                model_name=self.config.get("model_name", "gemini-1.5-flash-latest"),
                generation_config_dict=effective_gen_config
            )
            if isinstance(response_data, dict):
                return response_data
            elif isinstance(response_data, str):
                logger.warning("GeminiClientê°€ JSON ë¬¸ìì—´ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ (ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­). ì¬íŒŒì‹± ì‹œë„.")
                try:
                    return json.loads(response_data)
                except json.JSONDecodeError as e_json_reparse:
                    raise BtgTranslationException(f"ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e_json_reparse}. ì‘ë‹µ: {response_data[:200]}...", original_exception=e_json_reparse)
            else:
                raise BtgTranslationException(f"ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­ ìš”ì²­ì— ëŒ€í•œ ì˜ˆìƒì¹˜ ì•Šì€ ì‘ë‹µ íƒ€ì…: {type(response_data)}")
        except (BtgApiClientException, GeminiAllApiKeysExhaustedException) as e_api:
            logger.error(f"ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­ ì¤‘ API ì˜¤ë¥˜: {e_api}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            raise BtgTranslationException(f"ì§ì ‘ êµ¬ì¡°í™” ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", original_exception=e) from e

    def translate_text_with_content_safety_retry(
        self, 
        text_chunk: str, 
        max_split_attempts: int = 3,
        min_chunk_size: int = 100
    ) -> str:
        """
        ì½˜í…ì¸  ì•ˆì „ ì˜¤ë¥˜ ë°œìƒì‹œ ì²­í¬ë¥¼ ë¶„í• í•˜ì—¬ ì¬ì‹œë„í•˜ëŠ” ë²ˆì—­ ë©”ì„œë“œ
        
        Args:
            text_chunk: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            max_split_attempts: ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ ë¶€ë¶„ì€ ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ ëŒ€ì²´)
        """
        try:
            # 1ì°¨ ì‹œë„: ì „ì²´ ì²­í¬ ë²ˆì—­
            return self.translate_text(text_chunk)
            
        except BtgTranslationException as e:
            # ê²€ì—´ ì˜¤ë¥˜ê°€ ì•„ë‹Œ ê²½ìš° ê·¸ëŒ€ë¡œ ì˜ˆì™¸ ë°œìƒ
            if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" not in str(e):
                raise e
            
            logger.warning(f"ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ ê°ì§€. ì²­í¬ ë¶„í•  ì¬ì‹œë„ ì‹œì‘: {str(e)}")
            return self._translate_with_recursive_splitting(
                text_chunk, max_split_attempts, min_chunk_size, current_attempt=1
            )

    def _translate_with_recursive_splitting(
        self,
        text_chunk: str,
        max_split_attempts: int,
        min_chunk_size: int,
        current_attempt: int = 1
    ) -> str:
    
        if current_attempt > max_split_attempts:
            logger.error(f"ìµœëŒ€ ë¶„í•  ì‹œë„ íšŸìˆ˜({max_split_attempts})ì— ë„ë‹¬. ë²ˆì—­ ì‹¤íŒ¨.")
            return f"[ê²€ì—´ë¡œ ì¸í•œ ë²ˆì—­ ì‹¤íŒ¨: ìµœëŒ€ ë¶„í•  ì‹œë„ ì´ˆê³¼]"

        if len(text_chunk.strip()) <= min_chunk_size:
            logger.warning(f"ìµœì†Œ ì²­í¬ í¬ê¸°ì— ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ê²€ì—´ë¨: {text_chunk[:50]}...")
            return f"[ê²€ì—´ë¡œ ì¸í•œ ë²ˆì—­ ì‹¤íŒ¨: {text_chunk[:30]}...]"

        logger.info(f"ğŸ“Š ì²­í¬ ë¶„í•  ì‹œë„ #{current_attempt} (ê¹Šì´: {current_attempt-1})")
        logger.info(f"   ğŸ“ ì›ë³¸ í¬ê¸°: {len(text_chunk)} ê¸€ì")
        logger.info(f"   ğŸ¯ ëª©í‘œ í¬ê¸°: {len(text_chunk) // 2} ê¸€ì")
        logger.info(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {text_chunk[:100].replace(chr(10), ' ')}...")

        
        # 1ë‹¨ê³„: í¬ê¸° ê¸°ë°˜ ë¶„í• 
        sub_chunks = self.chunk_service.split_chunk_recursively(
            text_chunk,
            target_size=len(text_chunk) // 2,
            min_chunk_size=min_chunk_size,
            max_split_depth=1,  # 1ë‹¨ê³„ë§Œ ë¶„í• 
            current_depth=0
        )
        
        # ë¶„í• ì´ ì•ˆëœ ê²½ìš° ë¬¸ì¥ ê¸°ë°˜ ë¶„í•  ì‹œë„
        if len(sub_chunks) <= 1:
            logger.info("í¬ê¸° ê¸°ë°˜ ë¶„í•  ì‹¤íŒ¨. ë¬¸ì¥ ê¸°ë°˜ ë¶„í•  ì‹œë„.")
            sub_chunks = self.chunk_service.split_chunk_by_sentences(
                text_chunk, max_sentences_per_chunk=1
            )
        
        if len(sub_chunks) <= 1:
            logger.error("ì²­í¬ ë¶„í•  ì‹¤íŒ¨. ë²ˆì—­ í¬ê¸°.")
            return f"[ë¶„í•  ë¶ˆê°€ëŠ¥í•œ ê²€ì—´ ì½˜í…ì¸ : {text_chunk[:30]}...]"
        
        # ê° ì„œë¸Œ ì²­í¬ ê°œë³„ ë²ˆì—­ ì‹œë„
        translated_parts = []
        total_sub_chunks = len(sub_chunks)
        successful_sub_chunks = 0
        failed_sub_chunks = 0
        
        logger.info(f"ğŸ”„ ë¶„í•  ì™„ë£Œ: {total_sub_chunks}ê°œ ì„œë¸Œ ì²­í¬ ìƒì„±")
        
        for i, sub_chunk in enumerate(sub_chunks):
            sub_chunk_info = f"ì„œë¸Œ ì²­í¬ {i+1}/{total_sub_chunks}"
            sub_chunk_size = len(sub_chunk.strip())
            sub_chunk_preview = sub_chunk.strip()[:50].replace('\n', ' ') + '...'
            
            logger.info(f"   ğŸš€ {sub_chunk_info} ë²ˆì—­ ì‹œì‘")
            logger.debug(f"      ğŸ“ í¬ê¸°: {sub_chunk_size} ê¸€ì")
            logger.debug(f"      ğŸ“ ë‚´ìš©: {sub_chunk_preview}")
            
            start_time = time.time()
            
            try:
                translated_part = self.translate_text(sub_chunk.strip())
                processing_time = time.time() - start_time
                
                translated_parts.append(translated_part)
                successful_sub_chunks += 1
                
                logger.info(f"   âœ… {sub_chunk_info} ë²ˆì—­ ì„±ê³µ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
                logger.debug(f"      ğŸ“Š ê²°ê³¼ ê¸¸ì´: {len(translated_part)} ê¸€ì")
                logger.debug(f"      ğŸ“ˆ ì§„í–‰ë¥ : {(i+1)/total_sub_chunks*100:.1f}% ({i+1}/{total_sub_chunks})")
                
            except BtgTranslationException as sub_e:
                processing_time = time.time() - start_time
                
                if "ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ" in str(sub_e):
                    logger.warning(f"   ğŸ›¡ï¸ {sub_chunk_info} ê²€ì—´ ë°œìƒ (ì†Œìš”: {processing_time:.2f}ì´ˆ)")
                    logger.info(f"   ğŸ”„ ì¬ê·€ ë¶„í•  ì‹œë„ (ê¹Šì´: {current_attempt} â†’ {current_attempt+1})")
                    
                    # ì¬ê·€ì ìœ¼ë¡œ ë” ì‘ê²Œ ë¶„í•  ì‹œë„
                    recursive_result = self._translate_with_recursive_splitting(
                        sub_chunk, max_split_attempts, min_chunk_size, current_attempt + 1
                    )
                    translated_parts.append(recursive_result)
                    
                    if "[ê²€ì—´ë¡œ ì¸í•œ ë²ˆì—­ ì‹¤íŒ¨" in recursive_result:
                        failed_sub_chunks += 1
                        logger.warning(f"   âŒ {sub_chunk_info} ìµœì¢… ì‹¤íŒ¨")
                    else:
                        successful_sub_chunks += 1
                        logger.info(f"   âœ… {sub_chunk_info} ì¬ê·€ ë¶„í•  í›„ ì„±ê³µ")
                else:
                    # ë‹¤ë¥¸ ë²ˆì—­ ì˜¤ë¥˜ì¸ ê²½ìš°
                    failed_sub_chunks += 1
                    logger.error(f"   âŒ {sub_chunk_info} ë²ˆì—­ ì‹¤íŒ¨ (ì†Œìš”: {processing_time:.2f}ì´ˆ): {sub_e}")
                    translated_parts.append(f"[ë²ˆì—­ ì‹¤íŒ¨: {str(sub_e)}]")
                
                logger.debug(f"      ğŸ“ˆ ì§„í–‰ë¥ : {(i+1)/total_sub_chunks*100:.1f}% ({i+1}/{total_sub_chunks})")

        
        # ë²ˆì—­ëœ ë¶€ë¶„ë“¤ì„ ê²°í•©
        final_result = " ".join(translated_parts)
        
        # ë¶„í•  ë²ˆì—­ ì™„ë£Œ ìš”ì•½
        logger.info(f"ğŸ“‹ ë¶„í•  ë²ˆì—­ ì™„ë£Œ ìš”ì•½ (ê¹Šì´: {current_attempt-1})")
        logger.info(f"   ğŸ“Š ì´ ì„œë¸Œ ì²­í¬: {total_sub_chunks}ê°œ")
        logger.info(f"   âœ… ì„±ê³µ: {successful_sub_chunks}ê°œ")
        logger.info(f"   âŒ ì‹¤íŒ¨: {failed_sub_chunks}ê°œ")
        logger.info(f"   ğŸ“ ìµœì¢… ê²°ê³¼ ê¸¸ì´: {len(final_result)} ê¸€ì")
        
        if successful_sub_chunks > 0:
            success_rate = (successful_sub_chunks / total_sub_chunks) * 100
            logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return final_result
    




if __name__ == '__main__':
    # MockGeminiClientì—ì„œ typesë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì´ ë¸”ë¡ ë‚´ì—ì„œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
    from google.genai import types as genai_types # Ensure types is imported for hints
    
    print("--- TranslationService í…ŒìŠ¤íŠ¸ ---")
    class MockGeminiClient(GeminiClient):
        def __init__(self, auth_credentials, project=None, location=None, requests_per_minute: Optional[int] = None):
            try:
                super().__init__(auth_credentials=auth_credentials, project=project, location=location, requests_per_minute=requests_per_minute)
            except Exception as e:
                print(f"Warning: MockGeminiClient super().__init__ failed: {e}. This might be okay for some mock scenarios.")
                # If super init fails (e.g. dummy API key validation),
                # the mock might still function if it overrides all necessary methods
                # and doesn't rely on base class state initialized by __init__.
                # For Pylance, inheritance is the main fix.

            self.mock_auth_credentials = auth_credentials
            self.current_model_name_for_test: Optional[str] = None
            self.mock_api_keys_list: List[str] = []
            self.mock_current_api_key: Optional[str] = None

            if isinstance(auth_credentials, list):
                self.mock_api_keys_list = auth_credentials
                if self.mock_api_keys_list: self.mock_current_api_key = self.mock_api_keys_list[0]
            elif isinstance(auth_credentials, str) and not auth_credentials.startswith('{'): # Assuming API key string
                self.mock_api_keys_list = [auth_credentials]
                self.mock_current_api_key = auth_credentials
            print(f"MockGeminiClient initialized. Mock API Keys: {self.mock_api_keys_list}, Mock Current Key: {self.mock_current_api_key}")

        def generate_text(
            self,
            prompt: Union[str, List[Union[str, genai_types.Part]]],
            model_name: str,
            generation_config_dict: Optional[Dict[str, Any]] = None,
            safety_settings_list_of_dicts: Optional[List[Dict[str, Any]]] = None,
            system_instruction_text: Optional[str] = None,
            max_retries: int = 5,
            initial_backoff: float = 2.0,
            max_backoff: float = 60.0,
            stream: bool = False
        ) -> Optional[Union[str, Any]]:
            self.current_model_name_for_test = model_name

            prompt_text_for_mock = ""
            if isinstance(prompt, str):
                prompt_text_for_mock = prompt
            elif isinstance(prompt, list):
                temp_parts = []
                for item in prompt:
                    if isinstance(item, str):
                        temp_parts.append(item)
                    elif hasattr(item, 'text'): # Duck typing for Part-like objects
                        temp_parts.append(item.text)
                    else:
                        temp_parts.append(str(item))
                prompt_text_for_mock = "".join(temp_parts)

            print(f"  MockGeminiClient.generate_text í˜¸ì¶œë¨ (ëª¨ë¸: {model_name}). Mock í˜„ì¬ í‚¤: {self.mock_current_api_key[:5] if self.mock_current_api_key else 'N/A'}")

            if "ì•ˆì „ ë¬¸ì œ" in prompt_text_for_mock:
                raise GeminiContentSafetyException("Mock ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ")
            if "ì‚¬ìš©ëŸ‰ ì œí•œ" in prompt_text_for_mock: # Simplified logic for mock
                raise GeminiRateLimitException("Mock API ì‚¬ìš©ëŸ‰ ì œí•œ")
            if "ì˜ëª»ëœ ìš”ì²­" in prompt_text_for_mock:
                raise GeminiInvalidRequestException("Mock ì˜ëª»ëœ ìš”ì²­")

            text_to_be_translated = prompt_text_for_mock
            if "ë²ˆì—­í•  í…ìŠ¤íŠ¸:\n" in prompt_text_for_mock:
                text_to_be_translated = prompt_text_for_mock.split("ë²ˆì—­í•  í…ìŠ¤íŠ¸:\n")[-1].strip()
            elif "Translate to Korean:" in prompt_text_for_mock:
                 text_to_be_translated = prompt_text_for_mock.split("Translate to Korean:")[-1].strip()

            mock_translation = f"[ë²ˆì—­ë¨] {text_to_be_translated[:50]}..."

            is_json_response_expected = generation_config_dict and \
                                        generation_config_dict.get("response_mime_type") == "application/json"

            if is_json_response_expected:
                return {"translated_text": mock_translation, "mock_json": True}
            else:
                return mock_translation

        def list_models(self) -> List[Dict[str, Any]]:
            print("  MockGeminiClient.list_models í˜¸ì¶œë¨")
            # Return a structure similar to what GeminiClient.list_models would return
            return [
                {"name": "models/mock-gemini-flash", "short_name": "mock-gemini-flash", "display_name": "Mock Gemini Flash", "description": "A mock flash model.", "input_token_limit": 1000, "output_token_limit": 1000},
                {"name": "models/mock-gemini-pro", "short_name": "mock-gemini-pro", "display_name": "Mock Gemini Pro", "description": "A mock pro model.", "input_token_limit": 2000, "output_token_limit": 2000},
            ]

    sample_config_base = {
        "model_name": "gemini-1.5-flash", "temperature": 0.7, "top_p": 0.9,
        "prompts": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë¡œì–´ë¶ ì»¨í…ìŠ¤íŠ¸: {{lorebook_context}}\n\në²ˆì—­í•  í…ìŠ¤íŠ¸:\n{{slot}}",
        "enable_dynamic_lorebook_injection": True, # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í™œì„±í™”
        "lorebook_json_path": "test_lorebook.json", # í†µí•©ëœ ë¡œì–´ë¶ ê²½ë¡œ
        "max_lorebook_entries_per_chunk_injection": 3,
        "max_lorebook_chars_per_chunk_injection": 200,
    }

    # 1. ë¡œì–´ë¶ ì£¼ì… í…ŒìŠ¤íŠ¸
    print("\n--- 1. ë¡œì–´ë¶ ì£¼ì… ë²ˆì—­ í…ŒìŠ¤íŠ¸ ---")
    config1 = sample_config_base.copy()
    
    # í…ŒìŠ¤íŠ¸ìš© ë¡œì–´ë¶ íŒŒì¼ ìƒì„±
    test_lorebook_data = [
        {"keyword": "Alice", "description": "ì£¼ì¸ê³µ ì•¨ë¦¬ìŠ¤", "category": "ì¸ë¬¼", "importance": 10, "isSpoiler": False},
        {"keyword": "Bob", "description": "ì•¨ë¦¬ìŠ¤ì˜ ì¹œêµ¬ ë°¥", "category": "ì¸ë¬¼", "importance": 8, "isSpoiler": False}
    ]
    from file_handler import write_json_file, delete_file # write_csv_file -> write_json_file
    test_lorebook_file = Path("test_lorebook.json")
    if test_lorebook_file.exists(): delete_file(test_lorebook_file)
    write_json_file(test_lorebook_file, test_lorebook_data)

    gemini_client_instance = MockGeminiClient(auth_credentials="dummy_api_key")
    translation_service1 = TranslationService(gemini_client_instance, config1)
    text_to_translate1 = "Hello Alice, how are you Bob?"
    try:
        translated1 = translation_service1.translate_text(text_to_translate1)
        print(f"ì›ë³¸: {text_to_translate1}")
        print(f"ë²ˆì—­ ê²°ê³¼: {translated1}")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 1 ì˜¤ë¥˜: {e}")
    finally:
        if test_lorebook_file.exists(): delete_file(test_lorebook_file)

    # 2. ë¡œì–´ë¶ ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
    print("\n--- 2. ë¡œì–´ë¶ ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸ ---")
    config2 = sample_config_base.copy()
    config2["enable_dynamic_lorebook_injection"] = False
    translation_service2 = TranslationService(gemini_client_instance, config2)
    text_to_translate2 = "This is a test sentence."
    try:
        translated2 = translation_service2.translate_text(text_to_translate2)
        print(f"ì›ë³¸: {text_to_translate2}")
        print(f"ë²ˆì—­ ê²°ê³¼: {translated2}")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 2 ì˜¤ë¥˜: {e}")

    # 3. ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ í…ŒìŠ¤íŠ¸
    print("\n--- 3. ì½˜í…ì¸  ì•ˆì „ ë¬¸ì œ í…ŒìŠ¤íŠ¸ ---")
    config3 = sample_config_base.copy()
    translation_service3 = TranslationService(gemini_client_instance, config3)
    text_unsafe = "ì•ˆì „ ë¬¸ì œ í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸"
    try:
        translation_service3.translate_text(text_unsafe)
    except BtgTranslationException as e:
        print(f"ì˜ˆìƒëœ ì˜ˆì™¸ ë°œìƒ (ì½˜í…ì¸  ì•ˆì „): {e}")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ 3 ì˜¤ë¥˜: {type(e).__name__} - {e}")

    print("\n--- TranslationService í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ---")
